{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data sets\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv', encoding='ISO-8859-1', delimiter=';', on_bad_lines='skip')\n",
    "users = pd.read_csv('BX-Users.csv',encoding='ISO-8859-1', delimiter=';', on_bad_lines='skip')\n",
    "books = pd.read_csv('BX_Books.csv', encoding='ISO-8859-1', delimiter=';', quotechar='\"', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove images, year of publication, & publisher columns from Books dataset\n",
    "copy_books = books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'Year-Of-Publication','Publisher'], axis=1)\n",
    "copy_books.to_csv('new_books.csv', index=False, sep=';', encoding='ISO-8859-1', quotechar='\"')\n",
    "# #books csv to df\n",
    "pd.set_option('display.max_columns', None) #To print entire df\n",
    "pd.set_option('display.expand_frame_repr', False) #To print entire df\n",
    "books_df = pd.read_csv('new_books.csv', encoding='ISO-8859-1', delimiter=';', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "isbn_merge = pd.merge(ratings, books_df, on='ISBN')\n",
    "fin_merge = pd.merge(isbn_merge, users, on='User-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace na in Age column with median\n",
    "fin_merge['Age'] = fin_merge['Age'].fillna(fin_merge['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace any rows missing a value in location with n/a\n",
    "def clean_location(location):\n",
    "    parts = location.split(',')\n",
    "    if any(part.strip().lower() == 'n/a' for part in parts) or any(part.strip() == '' for part in parts):\n",
    "        return 'unknown'\n",
    "    return location\n",
    "fin_merge['Location'] = fin_merge['Location'].apply(clean_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace na in location with unknown\n",
    "fin_merge['Location'] = fin_merge['Location'].replace('n/a','unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplcate rows\n",
    "fin_merge.drop_duplicates(inplace=True)\n",
    "#create new csv file with cleaned data\n",
    "fin_merge.to_csv('merged_data.csv', index=False, sep=';', encoding='ISO-8859-1', quotechar='\"')\n",
    "print(fin_merge.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_data.csv', encoding='ISO-8859-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine book title and author columns for content-based filtering\n",
    "data['combined'] = data['Book-Title']+ \" \" + data['Book-Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the combined column\n",
    "tfdif = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfdif.fit_transform(data['combined'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ISBN to index\n",
    "isbn_to_index = {isbn: i for i, isbn in enumerate(data['ISBN'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering using Matrix Factorization\n",
    "# Create a pivot table\n",
    "user_item_matrix = data.pivot_table(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "# Decompose the matrix\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "user_factors = svd.fit_transform(user_item_matrix)\n",
    "item_factors = svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_id):\n",
    "    user_vector = user_factors[user_id-1]\n",
    "    predicted_ratings = np.dot(user_vector, item_factors)\n",
    "    return predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Collaborative and Content-based filtering\n",
    "def hybrid_rec(user_id, liked_isbn, top_n=5, alpha=0.5):\n",
    "    cf_ratings = predict_ratings(user_id)\n",
    "    cf_top_books = np.argsort(cf_ratings)[::-1][:top_n]\n",
    "    cf_book_ids = [user_item_matrix.columns[i] for i in cf_top_books]\n",
    "\n",
    "    if liked_isbn in isbn_to_index:\n",
    "        liked_book_index = isbn_to_index[liked_isbn]\n",
    "        content_similarities = cosine_sim[liked_book_index]\n",
    "        cb_top_books = np.argsort(content_similarities)[::-1][:top_n]\n",
    "        cb_book_ids = [list(isbn_to_index.keys())[i] for i in cb_top_books]\n",
    "    else:\n",
    "        cb_book_ids = []\n",
    "    \n",
    "    combined_recommendations = list(set(cf_book_ids+cb_book_ids))[:top_n]\n",
    "    return combined_recommendations\n",
    "print(hybrid_rec(1, '0345339703'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
